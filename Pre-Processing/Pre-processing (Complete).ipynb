{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a847dc05-60c4-4db4-a341-a5637ee28280",
   "metadata": {},
   "source": [
    "# Pre-processing (Complete)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349abfb3-8077-4c66-b879-2fc37bd4a208",
   "metadata": {},
   "source": [
    "Data Preprocessing: https://www.kaggle.com/khairulislam/data-preprocessing/notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522b1667-c473-4314-9671-e074b6e6a37f",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb1205f5-2252-4fbd-8055-079cda7c51e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sajepan\\.conda\\envs\\MasterThesisGPU\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\Sajepan\\.conda\\envs\\MasterThesisGPU\\lib\\site-packages\\numpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll\n",
      "C:\\Users\\Sajepan\\.conda\\envs\\MasterThesisGPU\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "# metrics are used to find accuracy or error\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c212e638-f911-46c1-bab8-dd51f470417d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd05b17d-d545-4c9a-90fb-05027b86331d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Reading Train and test dataset.\n",
    "# 2. Check if dataset is reversed.\n",
    "# 3. Drop 'id', and 'attack_cat' columns.\n",
    "def import_train_test():\n",
    "    train = pd.read_csv('../Dataset/UNSW_NB15_training-set.csv')\n",
    "    test = pd.read_csv('../Dataset/UNSW_NB15_testing-set.csv')\n",
    "    \n",
    "    # Dropping the columns based on Feature Selection:\n",
    "    # https://www.kaggle.com/khairulislam/unsw-nb15-feature-importance\n",
    "    drop_cols = ['attack_cat', 'id'] + ['response_body_len', 'spkts', 'ct_flw_http_mthd', 'trans_depth', 'dwin', 'ct_ftp_cmd', 'is_ftp_login']\n",
    "    for df in [train, test]:\n",
    "        for col in drop_cols:\n",
    "            if col in df.columns:\n",
    "                print('Dropping: ', col)\n",
    "                df.drop([col], axis=1, inplace=True)\n",
    "    \n",
    "    if train.shape < test.shape:\n",
    "        # Reversing the dataset\n",
    "        train, test = test, train\n",
    "        print(\"Train and Test sets are reversed, Corrected Shape:\")\n",
    "        print(\"Train shape: \", train.shape)\n",
    "        print(\"Test shape: \", test.shape)\n",
    "    else:\n",
    "        print(\"The dataset, is already reversed\")\n",
    "        print(\"Train shape: \", train.shape)\n",
    "        print(\"Test shape: \", test.shape)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8a06b05-28be-438f-80b6-6e593a0c0dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineer(df):\n",
    "    # Everything except: 'FIN', 'INT', 'CON', 'REQ', 'RST is renamed 'others'\n",
    "    df.loc[~df['state'].isin(['FIN', 'INT', 'CON', 'REQ', 'RST']), 'state'] = 'others'\n",
    "    # Everything except: ''-', 'dns', 'http', 'smtp', 'ftp-data', 'ftp', 'ssh', 'pop3' is renamed 'others'\n",
    "    df.loc[~df['service'].isin(['-', 'dns', 'http', 'smtp', 'ftp-data', 'ftp', 'ssh', 'pop3']), 'service'] = 'others'\n",
    "    # Merging 'igmp', 'icmp', 'rtp' into one protocol: 'igmp_icmp_rtp'\n",
    "    df.loc[df['proto'].isin(['igmp', 'icmp', 'rtp']), 'proto'] = 'igmp_icmp_rtp'\n",
    "    # Everything except: 'tcp', 'udp' ,'arp', 'ospf', 'igmp_icmp_rtp' is renamed to 'others'\n",
    "    df.loc[~df['proto'].isin(['tcp', 'udp','arp', 'ospf', 'igmp_icmp_rtp']), 'proto'] = 'others'\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c2e3114-55e4-4ab7-b1f1-dc4bf8f2e0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cat_columns(train):\n",
    "    # Defining an empty list\n",
    "    categorical = []\n",
    "    # Iterating through the columns and checking for columns with datatyp \"Object\"\n",
    "    for col in train.columns:\n",
    "        if train[col].dtype == 'object':\n",
    "            categorical.append(col) # appending \"object\" columns to categorical\n",
    "    return categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bd550f-c154-4770-8d28-fd45f121a27b",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bd4742c-865a-4056-969e-ee95799c11ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping:  attack_cat\n",
      "Dropping:  id\n",
      "Dropping:  response_body_len\n",
      "Dropping:  spkts\n",
      "Dropping:  ct_flw_http_mthd\n",
      "Dropping:  trans_depth\n",
      "Dropping:  dwin\n",
      "Dropping:  ct_ftp_cmd\n",
      "Dropping:  is_ftp_login\n",
      "Dropping:  attack_cat\n",
      "Dropping:  id\n",
      "Dropping:  response_body_len\n",
      "Dropping:  spkts\n",
      "Dropping:  ct_flw_http_mthd\n",
      "Dropping:  trans_depth\n",
      "Dropping:  dwin\n",
      "Dropping:  ct_ftp_cmd\n",
      "Dropping:  is_ftp_login\n",
      "Train and Test sets are reversed, Corrected Shape:\n",
      "Train shape:  (175341, 36)\n",
      "Test shape:  (82332, 36)\n"
     ]
    }
   ],
   "source": [
    "# Importing train test by using the function\n",
    "train, test = import_train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66bbc2e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dur                 0\n",
       "proto               0\n",
       "service             0\n",
       "state               0\n",
       "dpkts               0\n",
       "sbytes              0\n",
       "dbytes              0\n",
       "rate                0\n",
       "sttl                0\n",
       "dttl                0\n",
       "sload               0\n",
       "dload               0\n",
       "sloss               0\n",
       "dloss               0\n",
       "sinpkt              0\n",
       "dinpkt              0\n",
       "sjit                0\n",
       "djit                0\n",
       "swin                0\n",
       "stcpb               0\n",
       "dtcpb               0\n",
       "tcprtt              0\n",
       "synack              0\n",
       "ackdat              0\n",
       "smean               0\n",
       "dmean               0\n",
       "ct_srv_src          0\n",
       "ct_state_ttl        0\n",
       "ct_dst_ltm          0\n",
       "ct_src_dport_ltm    0\n",
       "ct_dst_sport_ltm    0\n",
       "ct_dst_src_ltm      0\n",
       "ct_src_ltm          0\n",
       "ct_srv_dst          0\n",
       "is_sm_ips_ports     0\n",
       "label               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To check if train and test datasets inhibits missing values\n",
    "train.isnull().sum()\n",
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "560645d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dur                 float64\n",
       "proto                object\n",
       "service              object\n",
       "state                object\n",
       "dpkts                 int64\n",
       "sbytes                int64\n",
       "dbytes                int64\n",
       "rate                float64\n",
       "sttl                  int64\n",
       "dttl                  int64\n",
       "sload               float64\n",
       "dload               float64\n",
       "sloss                 int64\n",
       "dloss                 int64\n",
       "sinpkt              float64\n",
       "dinpkt              float64\n",
       "sjit                float64\n",
       "djit                float64\n",
       "swin                  int64\n",
       "stcpb                 int64\n",
       "dtcpb                 int64\n",
       "tcprtt              float64\n",
       "synack              float64\n",
       "ackdat              float64\n",
       "smean                 int64\n",
       "dmean                 int64\n",
       "ct_srv_src            int64\n",
       "ct_state_ttl          int64\n",
       "ct_dst_ltm            int64\n",
       "ct_src_dport_ltm      int64\n",
       "ct_dst_sport_ltm      int64\n",
       "ct_dst_src_ltm        int64\n",
       "ct_src_ltm            int64\n",
       "ct_srv_dst            int64\n",
       "is_sm_ips_ports       int64\n",
       "label                 int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Addressing the different Data types for each column\n",
    "train.dtypes\n",
    "test.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea66bbf-377c-4e6d-b61f-7197e34d0d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into inputs and outputs\n",
    "x_train, y_train = train.drop(['label'], axis=1), train['label']\n",
    "x_test, y_test = test.drop(['label'], axis=1), test['label']\n",
    "# Running the inputs into the feature_engineer function\n",
    "x_train, x_test = feature_engineer(x_train), feature_engineer(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26a40bf-e871-4dfe-abd3-1a668ca92ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the categorical and non categorical columns\n",
    "categorical_columns = get_cat_columns(x_train)\n",
    "non_categorical_columns = [x for x in x_train.columns if x not in categorical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94611aa5-d30c-43d1-b420-bc76f6ce4fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using standard scaler to normalize data on non categorical columns\n",
    "scaler = StandardScaler()\n",
    "x_train[non_categorical_columns] = scaler.fit_transform(x_train[non_categorical_columns])\n",
    "x_test[non_categorical_columns] = scaler.transform(x_test[non_categorical_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecc3d8c-b3b9-4f9f-b8ed-7218e7102dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using get_dummies to make the categorical values usable.\n",
    "x_train = pd.get_dummies(x_train)\n",
    "x_test = pd.get_dummies(x_test)\n",
    "print(\"Column mismatch {0}, {1}\".format(set(x_train.columns)- set(x_test.columns),  set(x_test.columns)- set(x_train.columns)))\n",
    "features = list(set(x_train.columns) & set(x_test.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dd698e-04e4-4edc-a4ab-0144d02cac42",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(set(x_train.columns) & set(x_test.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a327fac7-135e-4d6d-a6a6-5ecf4cc38529",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of features {len(features)}\")\n",
    "x_train = x_train[features]\n",
    "x_test = x_test[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbe325d-1091-437c-b07c-ea0b61c910d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_train Shape: ', x_train.shape)\n",
    "print('y_train Shape: ', y_train.shape)\n",
    "print('X_test Shape: ', x_test.shape)\n",
    "print('y_test Shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f41f94-9697-45b9-9ed6-82c4cb99a973",
   "metadata": {},
   "source": [
    "## Export CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568a1d15-77b3-4939-8bca-644b98506f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge x_train and y_train before exporting to CSV\n",
    "x_train['label'] = y_train\n",
    "x_test['label'] = y_test\n",
    "x_train.to_csv('../Dataset/train_pp4.csv', index=False)\n",
    "x_test.to_csv('../Dataset/test_pp4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e226175-da21-434f-976f-2aaf22ed4993",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e806ced9ecfb40b02ab47568795ac9dcbe40749bdb5814bad29b2f2147c6506f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('MasterThesisGPU': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
