{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0bb492c-ad0e-4e68-84de-8c239108fe8c",
   "metadata": {},
   "source": [
    "# Feature Selection - XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9e3ce1-171f-475d-ab3c-25b9e0d289ff",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/feature-importance-and-feature-selection-with-xgboost-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee75095b-e89f-40d1-9f61-9facb7d82688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost\n",
    "from xgboost import plot_importance\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import preprocessing \n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fff32fe-2218-4cac-b771-788175716039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the training and testing datasets.\n",
    "test = pd.read_csv(\"UNSW_NB15_testing-set.csv\", sep=',', header=0)\n",
    "train = pd.read_csv(\"UNSW_NB15_training-set.csv\", sep=',', header=0)\n",
    "\n",
    "# Combining the testing and training dataset so we can split it more even.\n",
    "combined_data = pd.concat([train, test]).drop([\"id\"], axis = 1)\n",
    "\n",
    "# Splitting the datset into X and y\n",
    "X = combined_data.drop(['label', 'attack_cat'], axis=1)\n",
    "y = combined_data.loc[:, ['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5180929d-8fd2-4ca8-b6cc-2093c955d8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the columns that need to be label encoded.\n",
    "cols = ['proto', 'service', 'state']\n",
    "le = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37599e3a-df87-42fc-9d03-cac1786a07cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding the columns for the test and training set\n",
    "X[cols] = X[cols].apply(le.fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "937de1f6-be99-4ee6-b21b-ed95a0329bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL 2: Applying StandardScaler on X\n",
    "ss = StandardScaler()\n",
    "X = pd.DataFrame(ss.fit_transform(X),columns = X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6968b53-7f63-4284-a7fc-079c66812c68",
   "metadata": {},
   "source": [
    "## Feature selection - XGBoost Feature Importance Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "555f3af0-0320-467d-866a-807fbc1073fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:23:05] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', use_label_encoder=False,\n",
       "              validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model no training data\n",
    "model = XGBClassifier(use_label_encoder=False)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cae90ea-f441-4df5-af4d-64631aaa872c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "xgboost.plot_importance(model, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b4070d-d249-4ad4-940f-e8564be2e75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the files as png\n",
    "# fig.savefig('xgboost.png', figsize=(50, 40), dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a83e2c-973b-48b9-833b-63f5692e07be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
