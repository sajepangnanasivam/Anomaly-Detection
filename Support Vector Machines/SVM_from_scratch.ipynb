{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from UNSW_DFv2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_strength = 10000\n",
    "learning_rate = 0.000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = DF_preprocessed_traintest()\n",
    "X_train, X_test, y_train, y_test = DF_XY()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(train.index[10000:175341], axis=0, inplace=True)\n",
    "test.drop(test.index[5000:82332], axis=0, inplace=True)\n",
    "print(f\"Train shape:\\t {train.shape}\\nTest shape:\\t {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize():\n",
    "    print(\"training started \")\n",
    "    W = stochastic_gradient_descent(X_train.to_numpy(), y_train.to_numpy())\n",
    "    print(\"training finished.\")\n",
    "    print(\"weights are: {}\".format(W))\n",
    "    \n",
    "    y_test_predicted = np.array([])\n",
    "    for i in range(X_test.shape([0])):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_computation(W, X, Y):\n",
    "    #Calclate the hinge loss\n",
    "    N = X.shape[0]\n",
    "    distances = 1 - Y * (np.dot(X, W))\n",
    "    #equivalkent to max(0, distance)\n",
    "    distances[distances < 0] = 0\n",
    "    hinge_loss = reg_strength * (np.sum(distances) / N)\n",
    "    #Calculate the cost\n",
    "    cost = 1/2 *  np.dot(W, W) + hinge_loss\n",
    "    return cost\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_cost(W, X_batch, Y_batch):\n",
    "    if type(Y_batch) == np.float64:\n",
    "        Y_batch = np.array([Y_batch])\n",
    "        X_batch = np.array([X_batch])\n",
    "    \n",
    "    distance = 1 - (Y_batch * np.dot(X_batch, W))\n",
    "    dw = np.zeros(len(W))\n",
    "    \n",
    "    for ind, d in enumerate(distance):\n",
    "        if max(0, d) == 0:\n",
    "            di = W\n",
    "        else:\n",
    "            di = W - (reg_strength * Y_batch[ind] * X_batch[ind])\n",
    "        dw += di\n",
    "    \n",
    "    dw = dw/len(Y_batch)\n",
    "    return dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(features, outputs):\n",
    "    max_epochs = 5000\n",
    "    weights = np.zeros(features.shape[1])\n",
    "    nth = 0\n",
    "    prev_cost = float(\"inf\")\n",
    "    cost_threshold = 0.01\n",
    "    for epoch in range(1, max_epochs):\n",
    "        X, Y = shuffle(features, outputs)\n",
    "        for ind, x in enumerate(X):\n",
    "            ascent = gradient_cost(weights, x, Y[ind])\n",
    "            weights = weights - (learning_rate * ascent)\n",
    "    \n",
    "    \n",
    "    if epoch == 2 ** nth or epoch == max_epochs - 1:\n",
    "        cost = cost_computation(weights, features, outputs)\n",
    "        print(\"Epoch is: {} and cost is: {}\".format(epoch, cost))\n",
    "        if abs(prev_cost - cost) < cost_threshold * prev_cost:\n",
    "            return weights\n",
    "        prev_cost = cost\n",
    "        nth += 1\n",
    "\n",
    "    return weights"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
